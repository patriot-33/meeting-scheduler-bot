#!/usr/bin/env python3
"""
üõ°Ô∏è HOLISTIC PYTHON BACKEND DIAGNOSTIC & REPAIR SYSTEM v4.0
Senior Python Backend –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä —Å 15+ –ª–µ—Ç –æ–ø—ã—Ç–∞
–ê–ù–¢–ò–•–†–£–ü–ö–ê–Ø —Å–∏—Å—Ç–µ–º–∞ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ —Å –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ú —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –í–°–ï–ô –∏—Å—Ç–æ—Ä–∏–∏
"""

import json
import sqlite3
import pickle
import hashlib
import shutil
import asyncio
import traceback
import multiprocessing
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor
import subprocess
import os
import sys

# Core principles
DIAGNOSTIC_RULES = {
    "NEVER_ASSUME": "–ü—Ä–æ–≤–µ—Ä—è–π –∫–∞–∂–¥–æ–µ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–µ",
    "THINK_GLOBALLY": "–õ—é–±–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≤–ª–∏—è–µ—Ç –Ω–∞ –í–°–Æ —Å–∏—Å—Ç–µ–º—É",
    "INCREMENTAL_FIXES": "–ú–∞–ª–µ–Ω—å–∫–∏–µ —à–∞–≥–∏ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ",
    "PRESERVE_INVARIANTS": "–ù–µ –Ω–∞—Ä—É—à–∞–π —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–Ω—Ç—Ä–∞–∫—Ç—ã",
    "TEST_EVERYTHING": "–ù–µ–ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ = –Ω–æ–≤—ã–π –±–∞–≥",
    "DOCUMENT_EVERYTHING": "–ö–∞–∂–¥–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –î–û–õ–ñ–ù–û –±—ã—Ç—å –∑–∞–¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–æ",
    "NEVER_LOSE_HISTORY": "–ò—Å—Ç–æ—Ä–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π - —Å–≤—è—â–µ–Ω–Ω–∞, –ù–ò–ö–û–ì–î–ê –Ω–µ —Ç–µ—Ä—è–π –¥–∞–Ω–Ω—ã–µ"
}

class MandatoryHistoryPersistence:
    """–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –°–∏—Å—Ç–µ–º–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –í–°–ï–ô –∏—Å—Ç–æ—Ä–∏–∏"""
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.history_root = self.project_root / ".diagnostic_history"
        self.history_root.mkdir(exist_ok=True)
        
        # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
        self.db_path = self.history_root / "diagnostic_history.db"
        self.json_backup = self.history_root / "history_backup.json"
        self.pickle_backup = self.history_root / "history_backup.pkl"
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–î
        self._init_database()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥-—Ñ–∞–π–ª–∞
        self.log_file = self.history_root / "diagnostic.log"
        
    def _init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SQLite –ë–î –¥–ª—è –∏—Å—Ç–æ—Ä–∏–∏"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # –¢–∞–±–ª–∏—Ü–∞ —Å–µ—Å—Å–∏–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS diagnostic_sessions (
                session_id TEXT PRIMARY KEY,
                start_time TIMESTAMP,
                end_time TIMESTAMP,
                problem_description TEXT,
                final_status TEXT,
                full_data TEXT
            )
        """)
        
        # –¢–∞–±–ª–∏—Ü–∞ –≤—Å–µ—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS changes (
                change_id TEXT PRIMARY KEY,
                session_id TEXT,
                timestamp TIMESTAMP,
                file_path TEXT,
                change_type TEXT,
                diff_content TEXT,
                metrics_before TEXT,
                metrics_after TEXT,
                rollback_info TEXT,
                FOREIGN KEY (session_id) REFERENCES diagnostic_sessions(session_id)
            )
        """)
        
        # –¢–∞–±–ª–∏—Ü–∞ –±–∞–≥–æ–≤
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS bugs (
                bug_id TEXT PRIMARY KEY,
                session_id TEXT,
                discovery_time TIMESTAMP,
                bug_type TEXT,
                severity TEXT,
                root_cause TEXT,
                fix_applied TEXT,
                prevention_measures TEXT,
                FOREIGN KEY (session_id) REFERENCES diagnostic_sessions(session_id)
            )
        """)
        
        # –¢–∞–±–ª–∏—Ü–∞ –ø—Ä–æ–±–ª–µ–º
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS issues (
                issue_id TEXT PRIMARY KEY,
                session_id TEXT,
                timestamp TIMESTAMP,
                description TEXT,
                severity TEXT,
                component TEXT,
                fix_status TEXT
            )
        """)
        
        conn.commit()
        conn.close()
        
    def _generate_id(self, prefix: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ ID"""
        timestamp = datetime.now().isoformat()
        hash_part = hashlib.md5(f"{timestamp}{prefix}".encode()).hexdigest()[:8]
        return f"{prefix}_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{hash_part}"
    
    def _log(self, message: str, level: str = "INFO"):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º"""
        timestamp = datetime.now().isoformat()
        log_entry = f"[{timestamp}] [{level}] {message}\n"
        
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(log_entry)
        
        print(f"üîç [{level}] {message}")
    
    def save_session_start(self, session_data: Dict[str, Any]):
        """–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û–ï —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–∞—á–∞–ª–∞ —Å–µ—Å—Å–∏–∏"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–æ–≤ –∏ –¥–æ–±–∞–≤–∏—Ç—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        cursor.execute("PRAGMA table_info(diagnostic_sessions)")
        columns = [row[1] for row in cursor.fetchall()]
        
        if 'problem_description' not in columns:
            cursor.execute("ALTER TABLE diagnostic_sessions ADD COLUMN problem_description TEXT")
            conn.commit()
            
        if 'full_data' not in columns:
            cursor.execute("ALTER TABLE diagnostic_sessions ADD COLUMN full_data TEXT")
            conn.commit()
        
        cursor.execute("""
            INSERT INTO diagnostic_sessions (
                session_id, start_time, problem_description, full_data
            ) VALUES (?, ?, ?, ?)
        """, (
            session_data['session_id'],
            session_data['start_time'],
            session_data['problem_description'],
            json.dumps(session_data)
        ))
        
        conn.commit()
        conn.close()
        
        self._log(f"Session started: {session_data['session_id']}")
    
    def save_issue(self, issue_data: Dict[str, Any]) -> str:
        """–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û–ï —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã"""
        issue_id = self._generate_id("issue")
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            INSERT INTO issues (
                issue_id, session_id, timestamp, description,
                severity, component, fix_status
            ) VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (
            issue_id,
            issue_data.get('session_id'),
            datetime.now().isoformat(),
            issue_data.get('description'),
            issue_data.get('severity', 'medium'),
            issue_data.get('component'),
            'discovered'
        ))
        
        conn.commit()
        conn.close()
        
        self._log(f"Issue saved: {issue_id} - {issue_data.get('description')}")
        return issue_id

class SystemAnalyzer:
    """–ê–Ω–∞–ª–∏–∑ —Å–∏—Å—Ç–µ–º—ã —Å –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ–º –ø–æ–ª–Ω–æ–π –∫–∞—Ä—Ç—ã"""
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.history = MandatoryHistoryPersistence(project_root)
        
    def analyze_complete_system(self) -> Dict[str, Any]:
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –ø–æ–ª–Ω—É—é –∫–∞—Ä—Ç—É —Å–∏—Å—Ç–µ–º—ã –ü–ï–†–ï–î –ª—é–±—ã–º–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏"""
        self.history._log("–ù–∞—á–∏–Ω–∞—é –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–∏—Å—Ç–µ–º—ã")
        analysis_start = datetime.now()
        
        # 1. –ù–∞–π—Ç–∏ –≤—Å–µ Python —Ñ–∞–π–ª—ã
        python_files = list(self.project_root.rglob("*.py"))
        self.history._log(f"–ù–∞–π–¥–µ–Ω–æ Python —Ñ–∞–π–ª–æ–≤: {len(python_files)}")
        
        # 2. –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞
        project_structure = self.analyze_project_structure()
        
        # 3. –ù–∞–π—Ç–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∞–π–ª—ã
        critical_files = self.identify_critical_files()
        
        # 4. –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
        dependencies = self.analyze_dependencies()
        
        # 5. –ù–∞–π—Ç–∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –º–µ—Å—Ç–∞
        problem_patterns = self.detect_problem_patterns()
        
        analysis_result = {
            "timestamp": datetime.now().isoformat(),
            "total_python_files": len(python_files),
            "project_structure": project_structure,
            "critical_files": critical_files,
            "dependencies": dependencies,
            "problem_patterns": problem_patterns,
            "analysis_time": (datetime.now() - analysis_start).total_seconds()
        }
        
        # –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞
        analysis_file = self.history.history_root / "system_analysis.json"
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_result, f, indent=2, ensure_ascii=False)
        
        self.history._log(f"–ê–Ω–∞–ª–∏–∑ —Å–∏—Å—Ç–µ–º—ã –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {analysis_result['analysis_time']:.2f} —Å–µ–∫")
        return analysis_result
    
    def analyze_project_structure(self) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞"""
        structure = {
            "src_directory": str(self.project_root / "src"),
            "has_database": (self.project_root / "src" / "database.py").exists(),
            "has_services": (self.project_root / "src" / "services").exists(),
            "has_handlers": (self.project_root / "src" / "handlers").exists(),
            "has_config": (self.project_root / "src" / "config.py").exists(),
            "has_tests": (self.project_root / "tests").exists(),
            "has_requirements": (self.project_root / "requirements.txt").exists(),
            "has_docker": (self.project_root / "Dockerfile").exists()
        }
        
        return structure
    
    def identify_critical_files(self) -> List[str]:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–µ —Ñ–∞–π–ª—ã"""
        critical_patterns = [
            "database.py", "main.py", "config.py",
            "meeting_service.py", "google_calendar",
            "oauth_service.py"
        ]
        
        critical_files = []
        for pattern in critical_patterns:
            matches = list(self.project_root.rglob(f"*{pattern}*"))
            critical_files.extend([str(f) for f in matches])
        
        return critical_files
    
    def analyze_dependencies(self) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –ø—Ä–æ–µ–∫—Ç–∞"""
        dependencies = {
            "internal": [],
            "external": [],
            "database_related": [],
            "calendar_related": [],
            "telegram_related": []
        }
        
        # –ê–Ω–∞–ª–∏–∑ imports –≤ —Ñ–∞–π–ª–∞—Ö
        for py_file in self.project_root.rglob("*.py"):
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                # –ù–∞–π—Ç–∏ –∏–º–ø–æ—Ä—Ç—ã
                import_lines = [line.strip() for line in content.split('\n') 
                              if line.strip().startswith(('import ', 'from '))]
                
                for imp in import_lines:
                    if 'database' in imp.lower():
                        dependencies["database_related"].append(f"{py_file.name}: {imp}")
                    elif 'google' in imp.lower() or 'calendar' in imp.lower():
                        dependencies["calendar_related"].append(f"{py_file.name}: {imp}")
                    elif 'telegram' in imp.lower():
                        dependencies["telegram_related"].append(f"{py_file.name}: {imp}")
                        
            except Exception as e:
                self.history._log(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {py_file}: {e}", "ERROR")
        
        return dependencies
    
    def detect_problem_patterns(self) -> List[Dict[str, Any]]:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø—Ä–æ–±–ª–µ–º"""
        patterns = []
        
        # –ù–∞–π—Ç–∏ —Ñ–∞–π–ª—ã —Å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º –ª–æ–≥–∏–∫–∏
        calendar_files = list(self.project_root.rglob("*calendar*.py"))
        if len(calendar_files) > 1:
            patterns.append({
                "type": "potential_duplication",
                "description": f"–ù–∞–π–¥–µ–Ω–æ {len(calendar_files)} —Ñ–∞–π–ª–æ–≤ —Å calendar –≤ –∏–º–µ–Ω–∏",
                "files": [str(f) for f in calendar_files],
                "severity": "medium"
            })
        
        # –ù–∞–π—Ç–∏ –±–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã
        for py_file in self.project_root.rglob("*.py"):
            try:
                line_count = sum(1 for _ in open(py_file, 'r', encoding='utf-8'))
                if line_count > 500:
                    patterns.append({
                        "type": "large_file",
                        "description": f"–ë–æ–ª—å—à–æ–π —Ñ–∞–π–ª: {line_count} —Å—Ç—Ä–æ–∫",
                        "file": str(py_file),
                        "lines": line_count,
                        "severity": "low"
                    })
            except Exception:
                pass
        
        return patterns

class MeetingDuplicationAnalyzer:
    """–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Å—Ç—Ä–µ—á"""
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.history = MandatoryHistoryPersistence(project_root)
        
    def analyze_meeting_duplication(self) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–ª–µ–º—ã –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Å—Ç—Ä–µ—á"""
        self.history._log("–ù–∞—á–∏–Ω–∞—é –∞–Ω–∞–ª–∏–∑ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Å—Ç—Ä–µ—á")
        
        analysis = {
            "problems_detected": [],
            "root_causes": [],
            "affected_files": [],
            "fix_recommendations": []
        }
        
        # 1. –ù–∞–π—Ç–∏ –≤—Å–µ —Ñ–∞–π–ª—ã —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –∫–∞–ª–µ–Ω–¥–∞—Ä–µ–º
        calendar_files = self.find_calendar_related_files()
        analysis["affected_files"] = calendar_files
        
        # 2. –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–æ–∑–¥–∞–Ω–∏–µ –≤—Å—Ç—Ä–µ—á
        creation_analysis = self.analyze_meeting_creation()
        analysis["creation_analysis"] = creation_analysis
        
        # 3. –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —É–¥–∞–ª–µ–Ω–∏–µ –≤—Å—Ç—Ä–µ—á
        deletion_analysis = self.analyze_meeting_deletion()
        analysis["deletion_analysis"] = deletion_analysis
        
        # 4. –ù–∞–π—Ç–∏ –∫–æ—Ä–Ω–µ–≤—ã–µ –ø—Ä–∏—á–∏–Ω—ã
        root_causes = self.identify_root_causes(creation_analysis, deletion_analysis)
        analysis["root_causes"] = root_causes
        
        # 5. –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations = self.generate_fix_recommendations(root_causes)
        analysis["fix_recommendations"] = recommendations
        
        # –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∞–Ω–∞–ª–∏–∑
        analysis_file = self.history.history_root / "meeting_duplication_analysis.json"
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False)
        
        return analysis
    
    def find_calendar_related_files(self) -> List[str]:
        """–ù–∞–π—Ç–∏ –≤—Å–µ —Ñ–∞–π–ª—ã —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –∫–∞–ª–µ–Ω–¥–∞—Ä–µ–º"""
        patterns = ["*calendar*", "*meeting*", "*event*", "*google*"]
        files = []
        
        for pattern in patterns:
            matches = list(self.project_root.rglob(f"{pattern}.py"))
            files.extend([str(f) for f in matches])
        
        return files
    
    def analyze_meeting_creation(self) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –ª–æ–≥–∏–∫–∏ —Å–æ–∑–¥–∞–Ω–∏—è –≤—Å—Ç—Ä–µ—á"""
        self.history._log("–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –ª–æ–≥–∏–∫—É —Å–æ–∑–¥–∞–Ω–∏—è –≤—Å—Ç—Ä–µ—á")
        
        analysis = {
            "creation_methods": [],
            "potential_duplications": [],
            "calendar_services": []
        }
        
        # –ù–∞–π—Ç–∏ –≤—Å–µ –º–µ—Ç–æ–¥—ã —Å–æ–∑–¥–∞–Ω–∏—è
        for py_file in self.project_root.rglob("*.py"):
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # –ü–æ–∏—Å–∫ –º–µ—Ç–æ–¥–æ–≤ —Å–æ–∑–¥–∞–Ω–∏—è
                if 'create_meeting' in content:
                    analysis["creation_methods"].append({
                        "file": str(py_file),
                        "method": "create_meeting",
                        "line_count": content.count('create_meeting')
                    })
                
                if 'create_event' in content:
                    analysis["creation_methods"].append({
                        "file": str(py_file),
                        "method": "create_event", 
                        "line_count": content.count('create_event')
                    })
                
                # –ü–æ–∏—Å–∫ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
                if content.count('events().insert') > 1:
                    analysis["potential_duplications"].append({
                        "file": str(py_file),
                        "description": f"–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤—ã–∑–æ–≤—ã events().insert: {content.count('events().insert')}"
                    })
                
            except Exception as e:
                self.history._log(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {py_file}: {e}", "ERROR")
        
        return analysis
    
    def analyze_meeting_deletion(self) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –ª–æ–≥–∏–∫–∏ —É–¥–∞–ª–µ–Ω–∏—è –≤—Å—Ç—Ä–µ—á"""
        self.history._log("–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –ª–æ–≥–∏–∫—É —É–¥–∞–ª–µ–Ω–∏—è –≤—Å—Ç—Ä–µ—á")
        
        analysis = {
            "deletion_methods": [],
            "potential_issues": []
        }
        
        for py_file in self.project_root.rglob("*.py"):
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # –ü–æ–∏—Å–∫ –º–µ—Ç–æ–¥–æ–≤ —É–¥–∞–ª–µ–Ω–∏—è
                if 'cancel_meeting' in content or 'delete_meeting' in content:
                    analysis["deletion_methods"].append({
                        "file": str(py_file),
                        "has_cancel": 'cancel_meeting' in content,
                        "has_delete": 'delete_meeting' in content
                    })
                
                # –ü–æ–∏—Å–∫ –ø—Ä–æ–±–ª–µ–º —Å —É–¥–∞–ª–µ–Ω–∏–µ–º
                if 'events().delete' in content:
                    delete_count = content.count('events().delete')
                    if delete_count == 1:
                        analysis["potential_issues"].append({
                            "file": str(py_file),
                            "issue": "–¢–æ–ª—å–∫–æ –æ–¥–∏–Ω –≤—ã–∑–æ–≤ delete –¥–ª—è –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π"
                        })
                
            except Exception as e:
                self.history._log(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {py_file}: {e}", "ERROR")
        
        return analysis
    
    def identify_root_causes(self, creation_analysis: Dict, deletion_analysis: Dict) -> List[Dict[str, Any]]:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ—Ä–Ω–µ–≤—ã–µ –ø—Ä–∏—á–∏–Ω—ã –ø—Ä–æ–±–ª–µ–º"""
        root_causes = []
        
        # –ê–Ω–∞–ª–∏–∑ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏
        creation_methods_count = len(creation_analysis["creation_methods"])
        if creation_methods_count > 1:
            root_causes.append({
                "type": "multiple_creation_paths",
                "description": f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {creation_methods_count} —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ —Å–æ–∑–¥–∞–Ω–∏—è –≤—Å—Ç—Ä–µ—á",
                "severity": "high",
                "impact": "–ú–æ–∂–µ—Ç –ø—Ä–∏–≤–æ–¥–∏—Ç—å –∫ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–º—É —Å–æ–∑–¥–∞–Ω–∏—é —Å–æ–±—ã—Ç–∏–π"
            })
        
        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–ª–µ–º —Å —É–¥–∞–ª–µ–Ω–∏–µ–º
        if deletion_analysis["potential_issues"]:
            root_causes.append({
                "type": "incomplete_deletion",
                "description": "–ù–µ–ø–æ–ª–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ —Å–æ–±—ã—Ç–∏–π –∏–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∫–∞–ª–µ–Ω–¥–∞—Ä–µ–π",
                "severity": "high", 
                "impact": "–°–æ–±—ã—Ç–∏—è –æ—Å—Ç–∞—é—Ç—Å—è –≤ –∫–∞–ª–µ–Ω–¥–∞—Ä—è—Ö –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –≤ –±–æ—Ç–µ"
            })
        
        # –ê–Ω–∞–ª–∏–∑ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ–±—ã—Ç–∏–π
        for dup in creation_analysis["potential_duplications"]:
            root_causes.append({
                "type": "event_duplication_in_code",
                "description": f"–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤—ã–∑–æ–≤—ã events().insert –≤ {dup['file']}",
                "severity": "critical",
                "impact": "–ü—Ä—è–º–æ–µ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–±—ã—Ç–∏–π –≤ –∫–∞–ª–µ–Ω–¥–∞—Ä–µ"
            })
        
        return root_causes
    
    def generate_fix_recommendations(self, root_causes: List[Dict]) -> List[Dict[str, Any]]:
        """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—é"""
        recommendations = []
        
        for cause in root_causes:
            if cause["type"] == "multiple_creation_paths":
                recommendations.append({
                    "priority": "high",
                    "action": "consolidate_creation_logic",
                    "description": "–û–±—ä–µ–¥–∏–Ω–∏—Ç—å –≤—Å—é –ª–æ–≥–∏–∫—É —Å–æ–∑–¥–∞–Ω–∏—è –≤—Å—Ç—Ä–µ—á –≤ –æ–¥–∏–Ω –º–µ—Ç–æ–¥",
                    "steps": [
                        "–ù–∞–π—Ç–∏ –≤—Å–µ –º–µ—Å—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≤—Å—Ç—Ä–µ—á",
                        "–°–æ–∑–¥–∞—Ç—å –µ–¥–∏–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è",
                        "–†–µ—Ñ–∞–∫—Ç–æ—Ä–∏—Ç—å –≤—Å–µ –≤—ã–∑–æ–≤—ã —á–µ—Ä–µ–∑ –µ–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å"
                    ]
                })
            
            elif cause["type"] == "incomplete_deletion":
                recommendations.append({
                    "priority": "critical", 
                    "action": "fix_deletion_logic",
                    "description": "–ò—Å–ø—Ä–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É —É–¥–∞–ª–µ–Ω–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö –∫–∞–ª–µ–Ω–¥–∞—Ä–µ–π",
                    "steps": [
                        "–ù–∞–π—Ç–∏ –≤—Å–µ —Å–æ–∑–¥–∞–Ω–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è (manager + owner –∫–∞–ª–µ–Ω–¥–∞—Ä–∏)",
                        "–†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —É–¥–∞–ª–µ–Ω–∏–µ –∏–∑ –≤—Å–µ—Ö –∫–∞–ª–µ–Ω–¥–∞—Ä–µ–π",
                        "–î–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É —É—Å–ø–µ—à–Ω–æ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è"
                    ]
                })
            
            elif cause["type"] == "event_duplication_in_code":
                recommendations.append({
                    "priority": "critical",
                    "action": "remove_code_duplication", 
                    "description": "–£–±—Ä–∞—Ç—å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã–∑–æ–≤–æ–≤ events().insert",
                    "steps": [
                        "–ù–∞–π—Ç–∏ –≤—Å–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤—ã–∑–æ–≤—ã events().insert",
                        "–û—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –≤—ã–∑–æ–≤ –Ω–∞ –æ–¥–Ω–æ —Å–æ–±—ã—Ç–∏–µ",
                        "–†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–∞ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ"
                    ]
                })
        
        return recommendations

class GoogleMeetAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø—Ä–æ–±–ª–µ–º —Å Google Meet"""
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.history = MandatoryHistoryPersistence(project_root)
        
    def analyze_google_meet_issues(self) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–ª–µ–º —Å Google Meet"""
        self.history._log("–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –ø—Ä–æ–±–ª–µ–º—ã —Å Google Meet")
        
        analysis = {
            "conference_creation_methods": [],
            "oauth_vs_service_account": {},
            "potential_issues": []
        }
        
        # –ù–∞–π—Ç–∏ –≤—Å–µ –º–µ—Å—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è conferenceData
        for py_file in self.project_root.rglob("*.py"):
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # –ü–æ–∏—Å–∫ conferenceData
                if 'conferenceData' in content:
                    analysis["conference_creation_methods"].append({
                        "file": str(py_file),
                        "line_count": content.count('conferenceData'),
                        "has_create_request": 'createRequest' in content,
                        "has_conference_version": 'conferenceDataVersion' in content
                    })
                
                # –ê–Ω–∞–ª–∏–∑ OAuth vs Service Account
                if 'oauth' in content.lower():
                    analysis["oauth_vs_service_account"]["oauth_usage"] = True
                    
                if 'service_account' in content.lower() or 'service account' in content.lower():
                    analysis["oauth_vs_service_account"]["service_account_usage"] = True
                
            except Exception as e:
                self.history._log(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {py_file}: {e}", "ERROR")
        
        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
        if len(analysis["conference_creation_methods"]) > 1:
            analysis["potential_issues"].append({
                "type": "multiple_conference_creation",
                "description": "–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Å–æ–∑–¥–∞–Ω–∏—è Google Meet –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–π"
            })
        
        return analysis

async def diagnose_meeting_scheduler_problems(project_path: str) -> Dict[str, Any]:
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –ø—Ä–æ–±–ª–µ–º —Å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º –≤—Å—Ç—Ä–µ—á"""
    
    # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–∏—Å—Ç–µ–º—É –∏—Å—Ç–æ—Ä–∏–∏
    history = MandatoryHistoryPersistence(project_path)
    
    # –°–æ–∑–¥–∞—Ç—å —Å–µ—Å—Å–∏—é –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
    session_id = history._generate_id("session")
    session_data = {
        "session_id": session_id,
        "start_time": datetime.now().isoformat(),
        "problem_description": "–î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å—Ç—Ä–µ—á –∏ –ø—Ä–æ–±–ª–µ–º—ã —Å Google Meet",
        "project_path": project_path
    }
    
    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–∞—á–∞–ª–æ —Å–µ—Å—Å–∏–∏
    history.save_session_start(session_data)
    
    try:
        # 1. –û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ —Å–∏—Å—Ç–µ–º—ã
        history._log("=== PHASE 1: –ê–Ω–∞–ª–∏–∑ —Å–∏—Å—Ç–µ–º—ã ===")
        system_analyzer = SystemAnalyzer(project_path)
        system_analysis = system_analyzer.analyze_complete_system()
        
        # 2. –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Å—Ç—Ä–µ—á
        history._log("=== PHASE 2: –ê–Ω–∞–ª–∏–∑ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Å—Ç—Ä–µ—á ===")
        meeting_analyzer = MeetingDuplicationAnalyzer(project_path)
        meeting_analysis = meeting_analyzer.analyze_meeting_duplication()
        
        # 3. –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–ª–µ–º —Å Google Meet
        history._log("=== PHASE 3: –ê–Ω–∞–ª–∏–∑ Google Meet ===")
        meet_analyzer = GoogleMeetAnalyzer(project_path)
        meet_analysis = meet_analyzer.analyze_google_meet_issues()
        
        # 4. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø—Ä–æ–±–ª–µ–º—ã –≤ –±–∞–∑—É
        for cause in meeting_analysis.get("root_causes", []):
            history.save_issue({
                "session_id": session_id,
                "description": cause["description"],
                "severity": cause["severity"],
                "component": "meeting_system"
            })
        
        # 5. –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
        final_result = {
            "session_id": session_id,
            "timestamp": datetime.now().isoformat(),
            "status": "analysis_complete",
            "system_analysis": system_analysis,
            "meeting_duplication_analysis": meeting_analysis,
            "google_meet_analysis": meet_analysis,
            "critical_issues_count": len([c for c in meeting_analysis.get("root_causes", []) 
                                        if c.get("severity") == "critical"]),
            "fix_recommendations": meeting_analysis.get("fix_recommendations", [])
        }
        
        # –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result_file = history.history_root / f"diagnostic_result_{session_id}.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(final_result, f, indent=2, ensure_ascii=False)
        
        history._log(f"=== –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê ===")
        history._log(f"–ù–∞–π–¥–µ–Ω–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º: {final_result['critical_issues_count']}")
        history._log(f"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {result_file}")
        
        return final_result
        
    except Exception as e:
        # –ü—Ä–∏ –ª—é–±–æ–π –æ—à–∏–±–∫–µ - —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        error_data = {
            "session_id": session_id,
            "error": str(e),
            "traceback": traceback.format_exc(),
            "timestamp": datetime.now().isoformat()
        }
        
        error_file = history.history_root / f"diagnostic_error_{session_id}.json"
        with open(error_file, 'w', encoding='utf-8') as f:
            json.dump(error_data, f, indent=2, ensure_ascii=False)
        
        history._log(f"–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}", "ERROR")
        raise

if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
    import asyncio
    
    project_path = "/Users/evgenii/meeting-scheduler-bot"
    
    print("üõ°Ô∏è –ó–∞–ø—É—Å–∫–∞—é HOLISTIC DIAGNOSTIC SYSTEM v4.0")
    print("=" * 60)
    
    try:
        result = asyncio.run(diagnose_meeting_scheduler_problems(project_path))
        
        print("\n" + "=" * 60)
        print("‚úÖ –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û")
        print(f"üîç Session ID: {result['session_id']}")
        print(f"üö® –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º: {result['critical_issues_count']}")
        print(f"üìã –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {len(result['fix_recommendations'])}")
        
        # –ü–æ–∫–∞–∑–∞—Ç—å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
        if result['critical_issues_count'] > 0:
            print("\nüö® –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ü–†–û–ë–õ–ï–ú–´:")
            for i, cause in enumerate(result['meeting_duplication_analysis']['root_causes'], 1):
                if cause.get('severity') == 'critical':
                    print(f"  {i}. {cause['description']}")
        
        # –ü–æ–∫–∞–∑–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        print("\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ò–°–ü–†–ê–í–õ–ï–ù–ò–Æ:")
        for i, rec in enumerate(result['fix_recommendations'], 1):
            print(f"  {i}. [{rec['priority'].upper()}] {rec['description']}")
        
    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê –î–ò–ê–ì–ù–û–°–¢–ò–ö–ò: {e}")
        print(traceback.format_exc())